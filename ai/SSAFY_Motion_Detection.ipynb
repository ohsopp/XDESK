{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: mediapipe in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (0.10.7)\n",
      "Requirement already satisfied: scikit-learn in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: matplotlib in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from opencv-python) (1.23.5)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: absl-py in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model): \n",
    "    #image = feed frame\n",
    "    #model = Holistic model\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR -> RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB -> BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection_drawing_spec 커스텀 (optional)\n",
    "def draw_styled_landmarks(image, results):\n",
    "    #Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------custom function 실행------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_custom_pose_landmarks(image, results):\n",
    "    \n",
    "    pose_landmark_subset = landmark_pb2.NormalizedLandmarkList(\n",
    "      landmark = [\n",
    "          #eye keypoints\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE],#0 \n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE],#1\n",
    "          #right arm keypoints\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST],#2\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW],#3\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER],#4 \n",
    "          #left arm keypoints\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST],#5 \n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW],#6\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER],#7\n",
    "          #body keypoints\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP],#8\n",
    "          results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]])#9\n",
    "    \n",
    "    #connection = [pose_landmark_subset index1, pose_landmark_subset index2]\n",
    "    pose_landmark_subset_connections = [\n",
    "        [2, 3], [3, 4], #right arm\n",
    "        [5, 6], [6, 7], #left arm\n",
    "        [4, 7], [4, 9], [8, 9], [7, 8]] #body\n",
    "\n",
    "    #Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, \n",
    "                             landmark_list = pose_landmark_subset, \n",
    "                             connections = pose_landmark_subset_connections,\n",
    "                             landmark_drawing_spec = mp_drawing.DrawingSpec(color=(98,129,205), thickness=2, circle_radius=2), \n",
    "                             connection_drawing_spec = mp_drawing.DrawingSpec(color=(122,160,255), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_custom_hand_landmarks(image, results):\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(149,140,205), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(185,174,255), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(205,182,141), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(255,226,176), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_custom_face_landmarks(image, results):\n",
    "    #face landmark subset\n",
    "    face_landmark_subset = landmark_pb2.NormalizedLandmarkList(\n",
    "          landmark = [\n",
    "              #right eyebrow keypoints\n",
    "              results.face_landmarks.landmark[46],#0 \n",
    "              results.face_landmarks.landmark[53],#1\n",
    "              results.face_landmarks.landmark[52],#2\n",
    "              results.face_landmarks.landmark[65],#3\n",
    "              results.face_landmarks.landmark[55],#4\n",
    "\n",
    "              #left eyebrow keypoints\n",
    "              results.face_landmarks.landmark[285],#5 \n",
    "              results.face_landmarks.landmark[295],#6\n",
    "              results.face_landmarks.landmark[282],#7\n",
    "              results.face_landmarks.landmark[283],#8\n",
    "              results.face_landmarks.landmark[276],#9\n",
    "\n",
    "              #lip keypoints\n",
    "              results.face_landmarks.landmark[61],#10 \n",
    "              results.face_landmarks.landmark[81],#11\n",
    "              results.face_landmarks.landmark[13],#12 \n",
    "              results.face_landmarks.landmark[311],#13\n",
    "              results.face_landmarks.landmark[291],#14\n",
    "              results.face_landmarks.landmark[402],#15 \n",
    "              results.face_landmarks.landmark[14],#16\n",
    "              results.face_landmarks.landmark[178]])#17\n",
    "    \n",
    "    #connection = {face_landmark_subset index1, face_landmark_subset index2}\n",
    "    face_landmark_subset_connections = [\n",
    "        [0, 1], [1, 2], [2, 3], [3, 4], #right eybrow\n",
    "        [5, 6], [6, 7], [7, 8], [8, 9], #left eyebrow\n",
    "        [10, 11],[11, 12], [12, 13], [13, 14], [14, 15], [15, 16], [16, 17], [17, 10]] #lip\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, \n",
    "                             face_landmark_subset, \n",
    "                             face_landmark_subset_connections,\n",
    "                             mp_drawing.DrawingSpec(color=(112,190,205), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(139,236,255), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values\n",
    "results의 landmarks값 -> numpy array로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "poseIndex = np.array([22, 16, 14, 12, 24, 21, 15, 13, 11, 23])\n",
    "faceIndex = np.array([61, 291, 81, 13, 311, 178, 14, 402, 285, 295, 282, 283, 276, 46, 53, 52, 65, 55])\n",
    "\n",
    "def extract_keypoints_220(results):\n",
    "    # result의 landmarks의 모든 key point values -> 하나의 numpy array 로 flatten\n",
    "    #if landmarks has no value, fill numpy array with zero\n",
    "    \n",
    "    if results.pose_landmarks: #pose landmarks\n",
    "        #pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "        i = 0\n",
    "        pose = np.array([])\n",
    "        for res in results.pose_landmarks.landmark:\n",
    "            if i in poseIndex:\n",
    "                pose = np.append(pose, np.array([res.x, res.y, res.z, res.visibility]))\n",
    "            i = i + 1\n",
    "    else:\n",
    "        pose = np.zeros(132) #33*4\n",
    "\n",
    "    if results.left_hand_landmarks: #left hand landmarks\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        lh = np.zeros(63) #21*3\n",
    "\n",
    "    if results.right_hand_landmarks: #right hand landmarks\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        rh = np.zeros(63) #21*3\n",
    "        \n",
    "    #face landmarks\n",
    "    if results.face_landmarks:\n",
    "        #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()\n",
    "        i = 0\n",
    "        face = np.array([])\n",
    "        for res in results.face_landmarks.landmark:\n",
    "            if i in faceIndex:\n",
    "                face = np.append(face, np.array([res.x, res.y, res.z]))\n",
    "            i = i + 1\n",
    "    else:\n",
    "        face = np.zeros(54) #18*3\n",
    "    \n",
    "    \n",
    "    #return np.concatenate([pose, lh, rh])\n",
    "    return np.concatenate([pose, lh, rh, face])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_258(results):\n",
    "    # result의 landmarks의 모든 key point values -> 하나의 numpy array 로 flatten\n",
    "    #if landmarks has no value, fill numpy array with zero\n",
    "    \n",
    "    if results.pose_landmarks: #pose landmarks\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        pose = np.zeros(132) #33*4\n",
    "\n",
    "    if results.left_hand_landmarks: #left hand landmarks\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        lh = np.zeros(63) #21*3\n",
    "\n",
    "    if results.right_hand_landmarks: #right hand landmarks\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        rh = np.zeros(63) #21*3\n",
    "    \n",
    "    \n",
    "    #return np.concatenate([pose, lh, rh])\n",
    "    return np.concatenate([pose, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_1662(results):\n",
    "    # result의 landmarks의 모든 key point values -> 하나의 numpy array 로 flatten\n",
    "    #if landmarks has no value, fill numpy array with zero\n",
    "    \n",
    "    if results.pose_landmarks: #pose landmarks\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        pose = np.zeros(132) #33*4\n",
    "\n",
    "    if results.left_hand_landmarks: #left hand landmarks\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        lh = np.zeros(63) #21*3\n",
    "\n",
    "    if results.right_hand_landmarks: #right hand landmarks\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        rh = np.zeros(63) #21*3\n",
    "    \n",
    "    #face landmarks\n",
    "    if results.face_landmarks:\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() \n",
    "    else:\n",
    "        face = np.zeros(1404) #468*3\n",
    "    \n",
    "    #return np.concatenate([pose, lh, rh])\n",
    "    return np.concatenate([pose, lh, rh, face])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: openpyxl in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pandas in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ohsopp/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "!pip install xlrd\n",
    "!pip install openpyxl\n",
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가슴', '개', '귀', '내일', '누나', '다리', '동생', '뒤', '딸', '머리', '목', '물', '발', '배', '불', '뼈', '선생님', '손', '아기', '아내', '아들', '아빠', '앞', '어제', '엄마', '옆쪽', '오늘', '오른쪽', '왼쪽', '위', '친구', '코', '팔', '학교', 1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n"
     ]
    }
   ],
   "source": [
    "# 파일명\n",
    "file_name = '45.xlsx'\n",
    "\n",
    "# Daraframe형식으로 엑셀 파일 읽기\n",
    "df = pd.read_excel(file_name, sheet_name='Sheet1')\n",
    "\n",
    "list_res = df.loc[:,'단어'].to_list()\n",
    "print(list_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "#DATA_PATH = os.path.join('datas') \n",
    "DATA_PATH = os.path.join('ssafy_datas')\n",
    "\n",
    "#num of videos\n",
    "no_sequences = 60\n",
    "\n",
    "#actions = np.array(['가스', '가슴', '갇히다'])\n",
    "#actions = np.array(list_res)\n",
    "actions = np.array(['손흥민', '고양이', '볼하트', '최고', '걸어다닙니다'])\n",
    "\n",
    "# 1 Video = 50 frames\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##extract keypoints from video\n",
    "def frame_capture(action, sequence, video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        frame_num = 0\n",
    "        while cap.isOpened():\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "                \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)  \n",
    "            # Draw landmarks\n",
    "            #draw_styled_landmarks(image, results)\n",
    "            draw_custom_pose_landmarks(image, results)\n",
    "            draw_custom_hand_landmarks(image, results)\n",
    "            draw_custom_face_landmarks(image, results)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Video', image)\n",
    "\n",
    "            # export keypoints\n",
    "            keypoints = extract_keypoints(results) #results of frame -> numpy array\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints) #save numpy array in directory\n",
    "            \n",
    "            #increase frame_num\n",
    "            frame_num = frame_num + 1\n",
    "                \n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKeypoint(src):\n",
    "    poseIndex = np.array([22, 16, 14, 12, 24, 21, 15, 13, 11, 23])\n",
    "    faceIndex = np.array([61, 291, 81, 13, 311, 178, 14, 402, 285, 295, 282, 283, 276, 46, 53, 52, 65, 55])\n",
    "    result = np.array([])\n",
    "    #for p in poseIndex:\n",
    "    #    for i in range(0,4):\n",
    "    #        result = np.append(result, src[4*p + i])\n",
    "    #        \n",
    "    #result = np.append(result, src[133:133+126])\n",
    "    result = np.append(result, src[0:258])\n",
    "    \n",
    "    for f in faceIndex:\n",
    "        for i in range(0,3):\n",
    "            result = np.append(result, src[258 + 3*f + i])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------양손, 양손 + 포즈, 양손 + 포즈 + 얼굴 ------------------------\n",
    "\n",
    "import fnmatch #폴더 안에 있는 npy 개수 계산하기 위함\n",
    "\n",
    "#create two blank arrays\n",
    "sequences, labels = [], [] \n",
    "#sequences(=videos) = feature data = x data \n",
    "#labels = label data = y data\n",
    "\n",
    "for action in actions: \n",
    "    for sequence in range(no_sequences): #each action has 60 sequences(=videos)\n",
    "        window = []\n",
    "        \n",
    "        #폴더 안에 있는 npy 개수(= frame 수) 계산\n",
    "        file_count = len(fnmatch.filter(os.listdir(os.path.join(DATA_PATH, action, str(sequence))), '*.npy'))\n",
    "        \n",
    "        #저장된 npy 수가 원하는 sequence_length보다 같거나 많을 경우\n",
    "        if(file_count >= sequence_length):  \n",
    "            #앞에 있는 npy 버리고, start부터 끝까지 sequence_length개의 npy 가져오기\n",
    "            start = (file_count - sequence_length)//2\n",
    "            for frame_num in range(sequence_length):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(start + frame_num)))\n",
    "                #res_tmp = res[132:258]     # 양손만\n",
    "                #res_tmp = GetKeypoint(res)  # 포즈 + 손 + 일부 얼굴\n",
    "                res_tmp = res[0:258]    # 포즈 + 양손\n",
    "\n",
    "                window.append(res_tmp)\n",
    "        #저장된 npy 수가 원하는 sequence_length보다 적을 경우\n",
    "        else:\n",
    "            #앞에 0으로 채운 npy 추가하기\n",
    "            for frame_num in range(sequence_length - file_count):\n",
    "                res = np.zeros(258) #132 + 63 + 63\n",
    "                window.append(res)\n",
    "            for frame_num in range(file_count):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                window.append(res)\n",
    "                              \n",
    "        #map label\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 30, 258)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int) \n",
    "#converted label from int to binary array\n",
    "#1 -> [1,0,0], 2 -> [0,1,0], 3 -> [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "#split x,y data into train data and test data\n",
    "#train data 95%, test data size 5% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "#tensor board is used to monitor neural network training and it's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 30, 258)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 28, 64)               49600     ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 28, 64)               256       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 28, 64)               0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 14, 64)               0         ['re_lu_10[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 14, 64)               12352     ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 14, 64)               256       ['conv1d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 14, 64)               0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 14, 64)               12352     ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 14, 64)               256       ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 14, 64)               0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 14, 64)               12352     ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 14, 64)               256       ['conv1d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 14, 64)               0         ['batch_normalization_13[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 14, 64)               0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 7, 64)                0         ['re_lu_13[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 7, 64)                12352     ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 7, 64)                256       ['conv1d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 7, 64)                0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 7, 64)                12352     ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 7, 64)                256       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 7, 64)                0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 7, 64)                12352     ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 7, 64)                256       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 7, 64)                0         ['batch_normalization_16[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 7, 64)                0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 4, 64)                0         ['re_lu_16[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 4, 64)                12352     ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 4, 64)                256       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 4, 64)                0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 4, 64)                12352     ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 4, 64)                256       ['conv1d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 4, 64)                0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 4, 64)                12352     ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 4, 64)                256       ['conv1d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 4, 64)                0         ['batch_normalization_19[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 4, 64)                0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 2, 64)                0         ['re_lu_19[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  (None, 2, 128)               66048     ['max_pooling1d_7[0][0]']     \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 2, 128)               0         ['bidirectional_4[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirecti  (None, 2, 256)               263168    ['dropout_4[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 2, 256)               0         ['bidirectional_5[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirecti  (None, 2, 128)               164352    ['dropout_5[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 2, 128)               0         ['bidirectional_6[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirecti  (None, 2, 128)               98816     ['dropout_6[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 2, 128)               0         ['bidirectional_7[0][0]']     \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)               (None, 64)                   49408     ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 5)                    325       ['lstm_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 805445 (3.07 MB)\n",
      "Trainable params: 804165 (3.07 MB)\n",
      "Non-trainable params: 1280 (5.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 작동 O, model_trash6.h5\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Input, Embedding, GlobalAveragePooling1D, Conv1D, ReLU, MaxPooling1D, LSTM, Dense, Bidirectional, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "#Residual Block 구현\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    # Shortcut Connection\n",
    "    shortcut = x\n",
    "    \n",
    "    # Main Path\n",
    "    x = Conv1D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv1D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Shortcut Connection 추가\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(30,258))\n",
    "\n",
    "x = Conv1D(64, kernel_size=3, activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "x = residual_block(x, filters=64)\n",
    "x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "\n",
    "x = residual_block(x, filters=64)\n",
    "x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "\n",
    "x = residual_block(x, filters=64)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, activation='tanh'))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, activation='tanh'))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, activation='tanh'))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, activation='tanh'))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(64, return_sequences=False, activation='relu')(x)\n",
    "\n",
    "#output_layer = Dense(64, activation='relu')(x)\n",
    "#output_layer = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(actions.shape[0], activation='softmax')(x)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9/9 [==============================] - 8s 15ms/step - loss: 1.6057 - categorical_accuracy: 0.3018\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 1.5322 - categorical_accuracy: 0.7825\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 1.1204 - categorical_accuracy: 0.8035\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.4851 - categorical_accuracy: 0.9439\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.1830 - categorical_accuracy: 0.9754\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0084 - categorical_accuracy: 0.9965\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.2340 - categorical_accuracy: 0.9754\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.3895 - categorical_accuracy: 0.9474\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0618 - categorical_accuracy: 0.9754\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0489 - categorical_accuracy: 0.9860\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0092 - categorical_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0245 - categorical_accuracy: 0.9965\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0207 - categorical_accuracy: 0.9965\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0249 - categorical_accuracy: 0.9930\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0102 - categorical_accuracy: 0.9965\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0441 - categorical_accuracy: 0.9930\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0167 - categorical_accuracy: 0.9965\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0466 - categorical_accuracy: 0.9895\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0661 - categorical_accuracy: 0.9860\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0047 - categorical_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0058 - categorical_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0030 - categorical_accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0025 - categorical_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 6.2913e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 5.3408e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 4.7916e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 4.3374e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 4.7810e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 2.7940e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0064 - categorical_accuracy: 0.9965\n",
      "Epoch 33/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0037 - categorical_accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 2.8126e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 2.4850e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 2.9670e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 2.3913e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 2.1466e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.8572e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 6.5693e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.6829e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4118e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4053e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.1766e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0263 - categorical_accuracy: 0.9965\n",
      "Epoch 46/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0053 - categorical_accuracy: 0.9965\n",
      "Epoch 47/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 4.7910e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0747 - categorical_accuracy: 0.9895\n",
      "Epoch 50/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0042 - categorical_accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0602 - categorical_accuracy: 0.9930\n",
      "Epoch 52/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0963 - categorical_accuracy: 0.9789\n",
      "Epoch 53/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0043 - categorical_accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0103 - categorical_accuracy: 0.9965\n",
      "Epoch 55/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0115 - categorical_accuracy: 0.9965\n",
      "Epoch 56/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0558 - categorical_accuracy: 0.9860\n",
      "Epoch 57/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0084 - categorical_accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0331 - categorical_accuracy: 0.9965\n",
      "Epoch 59/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0020 - categorical_accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0172 - categorical_accuracy: 0.9965\n",
      "Epoch 63/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0073 - categorical_accuracy: 0.9965\n",
      "Epoch 64/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0411 - categorical_accuracy: 0.9895\n",
      "Epoch 65/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0128 - categorical_accuracy: 0.9965\n",
      "Epoch 66/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0036 - categorical_accuracy: 1.0000\n",
      "Epoch 67/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0628 - categorical_accuracy: 0.9860\n",
      "Epoch 68/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0694 - categorical_accuracy: 0.9825\n",
      "Epoch 69/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0333 - categorical_accuracy: 0.9930\n",
      "Epoch 70/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0085 - categorical_accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0326 - categorical_accuracy: 0.9895\n",
      "Epoch 72/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - categorical_accuracy: 1.0000\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 74/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0030 - categorical_accuracy: 1.0000\n",
      "Epoch 75/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 4.9660e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0019 - categorical_accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 4.9472e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0020 - categorical_accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 2.2201e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 2.2891e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0677 - categorical_accuracy: 0.9930\n",
      "Epoch 82/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0061 - categorical_accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0162 - categorical_accuracy: 0.9965\n",
      "Epoch 84/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0094 - categorical_accuracy: 0.9965\n",
      "Epoch 85/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 7.1807e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0042 - categorical_accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 3.4596e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 2.9762e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 3.3610e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 3.1069e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.8656e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 2.2867e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.2687e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.9629e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.6859e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 96/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.7864e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.8511e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.2440e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0709 - categorical_accuracy: 0.9860\n",
      "Epoch 100/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0168 - categorical_accuracy: 0.9930\n",
      "Epoch 101/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0048 - categorical_accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - categorical_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0018 - categorical_accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0417 - categorical_accuracy: 0.9965\n",
      "Epoch 105/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 7.6783e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 9.2063e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 5.3047e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 3.1859e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 111/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 2.4599e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 3.6105e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 2.0836e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 2.9297e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0213 - categorical_accuracy: 0.9965\n",
      "Epoch 116/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 7.0513e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0019 - categorical_accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 4.9491e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0121 - categorical_accuracy: 0.9965\n",
      "Epoch 120/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0036 - categorical_accuracy: 0.9965\n",
      "Epoch 121/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0036 - categorical_accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 5.6348e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0471 - categorical_accuracy: 0.9930\n",
      "Epoch 124/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0280 - categorical_accuracy: 0.9895\n",
      "Epoch 125/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0018 - categorical_accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0284 - categorical_accuracy: 0.9965\n",
      "Epoch 128/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0596 - categorical_accuracy: 0.9895\n",
      "Epoch 129/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0042 - categorical_accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0155 - categorical_accuracy: 0.9930\n",
      "Epoch 131/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0682 - categorical_accuracy: 0.9930\n",
      "Epoch 132/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0331 - categorical_accuracy: 0.9930\n",
      "Epoch 133/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0331 - categorical_accuracy: 0.9895\n",
      "Epoch 134/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 8.3285e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0061 - categorical_accuracy: 0.9965\n",
      "Epoch 137/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.1389 - categorical_accuracy: 0.9930\n",
      "Epoch 138/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0137 - categorical_accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0057 - categorical_accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0260 - categorical_accuracy: 0.9965\n",
      "Epoch 141/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0019 - categorical_accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 8.4732e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 144/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 28ms/step - loss: 5.0579e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 3.5078e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 3.0554e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 2.4136e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.8125e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.1132 - categorical_accuracy: 0.9860\n",
      "Epoch 150/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0425 - categorical_accuracy: 0.9930\n",
      "Epoch 151/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0189 - categorical_accuracy: 0.9965\n",
      "Epoch 152/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0019 - categorical_accuracy: 1.0000\n",
      "Epoch 153/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0246 - categorical_accuracy: 0.9965\n",
      "Epoch 154/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0359 - categorical_accuracy: 0.9930\n",
      "Epoch 155/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0027 - categorical_accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0070 - categorical_accuracy: 0.9965\n",
      "Epoch 157/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 158/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 4.7049e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 159/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 3.0791e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 2.2499e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 2.5175e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 2.7512e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 163/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 2.0229e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 164/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 2.5898e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.6281e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 166/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.3543e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 167/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.9248e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 168/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.0296e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 169/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4490e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 170/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 9.9326e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 171/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.1420e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 172/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.1323e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 173/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 9.8510e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 174/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 8.2902e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 175/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 2.4462e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 176/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 7.0686e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 177/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 9.5544e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 178/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 1.2336e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 179/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 7.4596e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 180/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 7.4388e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 181/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 7.8787e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 182/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 8.3374e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 183/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 9.9727e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 184/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 7.4263e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 185/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 7.6813e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 186/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0138 - categorical_accuracy: 0.9965\n",
      "Epoch 187/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 3.0465e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 188/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 8.3376e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 189/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 7.4375e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 190/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0092 - categorical_accuracy: 0.9965\n",
      "Epoch 191/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0052 - categorical_accuracy: 0.9965\n",
      "Epoch 192/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0031 - categorical_accuracy: 0.9965\n",
      "Epoch 193/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0051 - categorical_accuracy: 0.9965\n",
      "Epoch 194/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0081 - categorical_accuracy: 0.9965\n",
      "Epoch 195/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 3.6735e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 196/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0038 - categorical_accuracy: 1.0000\n",
      "Epoch 197/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 1.9648e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 198/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 7.2317e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 199/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 5.3161e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 200/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 5.7633e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 201/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 5.9945e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 202/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 4.7383e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 203/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 3.8812e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 204/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 6.4610e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 205/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 4.4174e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 206/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 4.6224e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 207/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 3.6543e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 208/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 3.5031e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 209/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 4.1618e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 210/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 3.9334e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 211/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 30, 258)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 28, 64)               49600     ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 28, 64)               256       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 28, 64)               0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 14, 64)               0         ['re_lu_10[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 14, 64)               12352     ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 14, 64)               256       ['conv1d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 14, 64)               0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 14, 64)               12352     ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 14, 64)               256       ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 14, 64)               0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 14, 64)               12352     ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 14, 64)               256       ['conv1d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 14, 64)               0         ['batch_normalization_13[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 14, 64)               0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 7, 64)                0         ['re_lu_13[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 7, 64)                12352     ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 7, 64)                256       ['conv1d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 7, 64)                0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 7, 64)                12352     ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 7, 64)                256       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 7, 64)                0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 7, 64)                12352     ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 7, 64)                256       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 7, 64)                0         ['batch_normalization_16[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 7, 64)                0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 4, 64)                0         ['re_lu_16[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 4, 64)                12352     ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 4, 64)                256       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 4, 64)                0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 4, 64)                12352     ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 4, 64)                256       ['conv1d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 4, 64)                0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 4, 64)                12352     ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 4, 64)                256       ['conv1d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 4, 64)                0         ['batch_normalization_19[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 4, 64)                0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 2, 64)                0         ['re_lu_19[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  (None, 2, 128)               66048     ['max_pooling1d_7[0][0]']     \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 2, 128)               0         ['bidirectional_4[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirecti  (None, 2, 256)               263168    ['dropout_4[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 2, 256)               0         ['bidirectional_5[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirecti  (None, 2, 128)               164352    ['dropout_5[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 2, 128)               0         ['bidirectional_6[0][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirecti  (None, 2, 128)               98816     ['dropout_6[0][0]']           \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 2, 128)               0         ['bidirectional_7[0][0]']     \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)               (None, 64)                   49408     ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 5)                    325       ['lstm_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 805445 (3.07 MB)\n",
      "Trainable params: 804165 (3.07 MB)\n",
      "Non-trainable params: 1280 (5.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "predict_res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohsopp/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#단일 file/folder 에 모델의 아키텍처, 가중치 및 훈련 구성을 저장\n",
    "# pose + hands : model_44_258.h5\n",
    "# hands : model_44_126.h5\n",
    "# pose 일부 + hands + face 일부 : model_44.220.h5\n",
    "# pose + hands + face 일부 : model_44_312.h5\n",
    "\n",
    "#model.save('model_44_220.h5')\n",
    "model.save('ssafy_model_258.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('model_44_258.h5')\n",
    "model.load_weights('ssafy_model_258.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test의 값들이 가리키는 label들을 하나의 리스트로 변환\n",
    "#ex. [[0, 0, 1], [0, 1, 0]] -> [2, 1]\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "#yhat의 값들이 가리키는 label들을 하나의 리스트로 변환\n",
    "#ex. [[0.1, 0.2, 0.7], [0.1, 0.8, 0.1]] -> [2, 1]\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[11,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[13,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[13,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[12,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[11,  0],\n",
       "        [ 0,  4]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns a confusion matrix sorted by the label order\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-1. Test using video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 텍스트 출력\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "def putKoreanText(src, text, pos, font_size, font_color):\n",
    "    img_pil = Image.fromarray(src)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    font = ImageFont.truetype('/Users/ohsopp/Desktop/tensorflow/ActionDetectionforSignLanguage/fonts/gulim.ttc', font_size)\n",
    "    draw.text(pos, text, font=font, fill= font_color)\n",
    "    return np.array(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using video\n",
    "def video_prediction(video):\n",
    "    \n",
    "    # 1. detection variables\n",
    "    sequence = [] #collect 60 frames to make a sequence(=video)\n",
    "    sentence = [] #concatenate history of predictions together\n",
    "    threshold = 0.995\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    #clear subtitles when {clear_cycle} frames passed without new subtitle added\n",
    "    frames_without_new_subtitle = 0\n",
    "    clear_cycle = 300\n",
    "    \n",
    "    word = ''\n",
    "    cnt = 0\n",
    "    \n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Read video\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "                \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            print(results)\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-60:] #generate sequence with last 30 frames\n",
    "            \n",
    "            #frames_without_new_subtitle += 1\n",
    "\n",
    "            if len(sequence) == 60:\n",
    "                #sequence.shape = (60, 1662)\n",
    "                #the input shape model expects = (number of sequences, 60, 1662)\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(actions[np.argmax(res)])\n",
    "                \n",
    "                if word == actions[np.argmax(res)]:\n",
    "                    cnt = cnt + 1\n",
    "                else :\n",
    "                    word = actions[np.argmax(res)]\n",
    "                    cnt = 0\n",
    "                    \n",
    "\n",
    "                #3. Rendering logic\n",
    "                #ex. res = [0.1, 0.2, 0.7]\n",
    "                #np.argmax(res) = 2, res[np.argmax(res)] = 0.7\n",
    "                if res[np.argmax(res)] > threshold and cnt >= 30: \n",
    "                    #new subtitle added\n",
    "                    #frames_without_new_subtitle = 0\n",
    "\n",
    "                    cur_action_korean = actions[np.argmax(res)]\n",
    "\n",
    "                    if len(sentence) > 0: \n",
    "                        #sentence에 저장된 prediction이 있는 경우 \n",
    "                        #새로운 prediction인 경우에만 sentence에 추가\n",
    "                        if cur_action_korean != sentence[-1]:\n",
    "                            sentence.append(cur_action_korean)\n",
    "                    else: \n",
    "                        #sentence에 저장된 prediction 없는 경우 바로 sentence에 추가\n",
    "                        sentence.append(cur_action_korean)\n",
    "\n",
    "                #sentence가 너무 길어지지 않도록 마지막 5개의 prediction만 유지\n",
    "                if len(sentence) > 5: \n",
    "                    sentence = sentence[-5:]\n",
    "                \n",
    "                #Clear subtitles if needed\n",
    "#                if frames_without_new_subtitle >= clear_cycle:\n",
    "#                    sentence.clear()\n",
    "                \n",
    "                #Render subtitles\n",
    "                cv2.rectangle(image, (0,0), (640, 40), (0, 0, 0), -1) \n",
    "#                if target == 'ko':\n",
    "#                    #putKoreanText(src, text, pos, font_size, font_color\n",
    "#                    image = putKoreanText(image, ' '.join(sentence), (3,10), 20, (255, 255, 255))\n",
    "#                else:\n",
    "#                    cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "                image = putKoreanText(image, ' '.join(sentence), (3,10),(20),(255,255,255))\n",
    "                # Show to screen\n",
    "                cv2.imshow('Video Prediction', image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using video\n",
    "def custom_video_prediction(video):\n",
    "    \n",
    "    # 1. detection variables\n",
    "    sequence = [] #collect 60 frames to make a sequence(=video)\n",
    "    sentence = [] #concatenate history of predictions together\n",
    "    threshold = 0.995\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    #clear subtitles when {clear_cycle} frames passed without new subtitle added\n",
    "    frames_without_new_subtitle = 0\n",
    "    clear_cycle = 300\n",
    "    \n",
    "    word = ''\n",
    "    cnt = 0\n",
    "    frame_num = 0\n",
    "    \n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # Read video\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "                \n",
    "            frame_num = frame_num+1\n",
    "            \n",
    "            if frame_num%2 == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            print(results)\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "            #draw_custom_pose_landmarks(image, results)\n",
    "            #draw_custom_hand_landmarks(image, results)\n",
    "            #draw_custom_face_landmarks(image, results)\n",
    "\n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints_258(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:] #generate sequence with last 30 frames\n",
    "            \n",
    "            #frames_without_new_subtitle += 1\n",
    "\n",
    "            if len(sequence) == 30:\n",
    "                #sequence.shape = (60, 1662)\n",
    "                #the input shape model expects = (number of sequences, 60, 1662)\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(actions[np.argmax(res)])\n",
    "                \n",
    "                if word == actions[np.argmax(res)]:\n",
    "                    cnt = cnt + 1\n",
    "                else :\n",
    "                    word = actions[np.argmax(res)]\n",
    "                    cnt = 1\n",
    "                    \n",
    "\n",
    "                #3. Rendering logic\n",
    "                #ex. res = [0.1, 0.2, 0.7]\n",
    "                #np.argmax(res) = 2, res[np.argmax(res)] = 0.7\n",
    "                if res[np.argmax(res)] > threshold and cnt >= 10: \n",
    "                    #new subtitle added\n",
    "                    #frames_without_new_subtitle = 0\n",
    "\n",
    "                    cur_action_korean = actions[np.argmax(res)]\n",
    "\n",
    "                    if len(sentence) > 0: \n",
    "                        #sentence에 저장된 prediction이 있는 경우 \n",
    "                        #새로운 prediction인 경우에만 sentence에 추가\n",
    "                        if cur_action_korean != sentence[-1]:\n",
    "                            sentence.append(cur_action_korean)\n",
    "                    else: \n",
    "                        #sentence에 저장된 prediction 없는 경우 바로 sentence에 추가\n",
    "                        sentence.append(cur_action_korean)\n",
    "\n",
    "                #sentence가 너무 길어지지 않도록 마지막 5개의 prediction만 유지\n",
    "                if len(sentence) > 5: \n",
    "                    sentence = sentence[-5:]\n",
    "                \n",
    "                #Clear subtitles if needed\n",
    "#                if frames_without_new_subtitle >= clear_cycle:\n",
    "#                    sentence.clear()\n",
    "                \n",
    "                #Render subtitles\n",
    "                cv2.rectangle(image, (0,0), (640, 80), (0, 0, 0), -1) \n",
    "#                if target == 'ko':\n",
    "#                    #putKoreanText(src, text, pos, font_size, font_color\n",
    "#                    image = putKoreanText(image, ' '.join(sentence), (3,10), 20, (255, 255, 255))\n",
    "#                else:\n",
    "#                    cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "                image = putKoreanText(image, ' '.join(sentence), (3,10),(50),(255,255,255))\n",
    "                # Show to screen\n",
    "                cv2.imshow('Video Prediction', image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "아내\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "아내\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "선생님\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "아기\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "아기\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "학교\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "아기\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "아기\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "친구\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "딸\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "목\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "왼쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "오른쪽\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "동생\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "동생\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "동생\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "2\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "3\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "3\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "동생\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "동생\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m      5\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(curDir, file)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mcustom_video_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 34\u001b[0m, in \u001b[0;36mcustom_video_prediction\u001b[0;34m(video)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Make detections\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m image, results \u001b[38;5;241m=\u001b[39m \u001b[43mmediapipe_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Draw landmarks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mmediapipe_detection\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;66;03m# COLOR CONVERSION BGR -> RGB\u001b[39;00m\n\u001b[1;32m      5\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m                  \u001b[38;5;66;03m# Image is no longer writeable\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m                 \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m                   \u001b[38;5;66;03m# Image is now writeable \u001b[39;00m\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR) \u001b[38;5;66;03m# COLOR COVERSION RGB -> BGR\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/mediapipe/python/solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curDir = \"/Users/ohsopp/Desktop/tensorflow/ActionDetectionforSignLanguage/졸프시연단어\"\n",
    "for i in range(100):\n",
    "    for file in os.listdir(curDir):\n",
    "        if(file.endswith(\".mp4\")):\n",
    "            path = os.path.join(curDir, file)\n",
    "            custom_video_prediction(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-2. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_holistic\u001b[38;5;241m.\u001b[39mHolistic(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m holistic:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Read feed\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. detection variables\n",
    "sequence = [] #collect 60 frames to make a sequence(=video)\n",
    "sentence = [] #concatenate history of predictions together\n",
    "threshold = 0.995\n",
    "\n",
    "word = ''\n",
    "cnt = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints_258(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:] #generate sequence with last 30 frames\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            #sequence.shape = (60, 1662)\n",
    "            #the input shape model expects = (number of sequences, 60, 1662)\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "\n",
    "            if word == actions[np.argmax(res)]:\n",
    "                cnt = cnt + 1\n",
    "            else :\n",
    "                word = actions[np.argmax(res)]\n",
    "                cnt = 1\n",
    "\n",
    "\n",
    "            #3. Rendering logic\n",
    "            #ex. res = [0.1, 0.2, 0.7]\n",
    "            #np.argmax(res) = 2, res[np.argmax(res)] = 0.7\n",
    "            if res[np.argmax(res)] > threshold and cnt >= 10: \n",
    "                #new subtitle added\n",
    "                #frames_without_new_subtitle = 0\n",
    "\n",
    "                cur_action_korean = actions[np.argmax(res)]\n",
    "\n",
    "                if len(sentence) > 0: \n",
    "                    #sentence에 저장된 prediction이 있는 경우 \n",
    "                    #새로운 prediction인 경우에만 sentence에 추가\n",
    "                    if cur_action_korean != sentence[-1]:\n",
    "                        sentence.append(cur_action_korean)\n",
    "                else: \n",
    "                    #sentence에 저장된 prediction 없는 경우 바로 sentence에 추가\n",
    "                    sentence.append(cur_action_korean)\n",
    "\n",
    "            #sentence가 너무 길어지지 않도록 마지막 5개의 prediction만 유지\n",
    "            if len(sentence) > 1: \n",
    "                sentence = sentence[-1:]\n",
    "                #print(sentence)\n",
    "\n",
    "            #Clear subtitles if needed\n",
    "#                if frames_without_new_subtitle >= clear_cycle:\n",
    "#                    sentence.clear()\n",
    "\n",
    "            #Render subtitles\n",
    "            cv2.rectangle(image, (0,0), (640, 80), (0, 0, 0), -1) \n",
    "#                if target == 'ko':\n",
    "#                    #putKoreanText(src, text, pos, font_size, font_color\n",
    "#                    image = putKoreanText(image, ' '.join(sentence), (3,10), 20, (255, 255, 255))\n",
    "#                else:\n",
    "#                    cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "            image = putKoreanText(image, ' '.join(sentence), (3,10),(50),(255,255,255))\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
